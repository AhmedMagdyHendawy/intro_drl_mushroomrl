{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](fig/acrobot.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration\n",
    "\n",
    "![Alt Text](fig/RL_illustration.png)\n",
    "\n",
    "*Reference: https://lilianweng.github.io/posts/2018-02-19-rl-overview/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "MDP = $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, R, \\rho, \\gamma>$\n",
    "\n",
    "- State s $\\in \\mathcal{S}$\n",
    "- Action a $\\in \\mathcal{A}$\n",
    "- Transition Function $ s^{'} \\sim P(s^{'}| s, a) $\n",
    "- Reward function $r = R(s,a,s^{'})$\n",
    "- Initial State Transition Function $s_{0} \\sim \\rho$\n",
    "- Discount Factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Episode $\\tau = (s_{0}, a_{0}, r_{1}, s_{1}, a_{1}, r_{2}, ... )$\n",
    "\n",
    "$ G = r_{1} + \\gamma r_{2} + \\gamma^{2} r_{3} + ... = \\sum_{t=0}^{\\infty} \\gamma^{t} r_{t+1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy (Actor)\n",
    "\n",
    "Agent uses a policy to make actions.\n",
    "\n",
    "$ a \\sim \\pi(a|s) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function (Critic)\n",
    "\n",
    "Value function helps the agent evaluate actions.\n",
    "\n",
    "Value function: $v(s) = \\mathbb{E}_{\\pi} [G_{t} | s_{t}=s]$\n",
    "\n",
    "Action-value function: $q(s,a) = \\mathbb{E}_{\\pi} [G_{t} | s_{t}=s, a_{t}=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "- **Episode**: transition with finite time $T$\n",
    "- **Horizon**: $T$ steps\n",
    "- **Epoch**: learn + evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mushroom RL\n",
    "\n",
    "![](fig/mushroom_rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three basic interface of mushroom_rl are the Agent, the Environment and the Core interface.\n",
    "\n",
    "- The **Agent** is the basic interface for any Reinforcement Learning algorithm.\n",
    "\n",
    "- The **Environment** is the basic interface for every problem/task that the agent should solve.\n",
    "\n",
    "- The **Core** is a class used to control the interaction between an agent and an environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "/home/hendawy/anaconda3/envs/mushroom_rl/lib/python3.8/site-packages/gym/envs/registration.py:415: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "pybullet build time: Nov 28 2023 23:51:11\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mushroom_rl.algorithms.value import DQN\n",
    "from mushroom_rl.core import Core, Logger\n",
    "from mushroom_rl.environments import *\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.approximators.parametric.torch_approximator import *\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from mushroom_rl.utils.parameters import Parameter, LinearParameter\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 1000\n",
    "gamma = 0.99\n",
    "gamma_eval = 1.\n",
    "mdp = Gym('Acrobot-v1', horizon, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ a_{greedy} = \\text{argmax}_{a}~q(s,a)$\n",
    "\n",
    "$\\pi(a|s) = \n",
    "\\left\\{\\begin{matrix}\n",
    "a_{greedy} & \\text{with}~1-\\epsilon\\\\ \n",
    "a_{random} & \\text{with}~\\epsilon\n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = LinearParameter(value=1., threshold_value=.01, n=5000)\n",
    "epsilon_test = Parameter(value=0.)\n",
    "epsilon_random = Parameter(value=1.)\n",
    "pi = EpsGreedy(epsilon=epsilon_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "n_epochs=20\n",
    "n_steps=1000\n",
    "n_steps_test=2000\n",
    "\n",
    "initial_replay_size = 500\n",
    "max_replay_size = 5000\n",
    "target_update_frequency = 100\n",
    "batch_size = 200\n",
    "n_features = 80\n",
    "train_frequency = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "\n",
    "# q(s, .) = [q1, q2, q3]\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n",
    "        features2 = F.relu(self._h2(features1))\n",
    "        q = self._h3(features2)\n",
    "\n",
    "        if action is None:\n",
    "            return q\n",
    "        else:\n",
    "            action = action.long()\n",
    "            q_acted = torch.squeeze(q.gather(1, action))\n",
    "\n",
    "            return q_acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximator\n",
    "input_shape = mdp.info.observation_space.shape\n",
    "approximator_params = dict(network=Network,\n",
    "                            optimizer={'class': optim.Adam,\n",
    "                                        'params': {'lr': .001}},\n",
    "                            loss=F.smooth_l1_loss,\n",
    "                            n_features=n_features,\n",
    "                            input_shape=input_shape,\n",
    "                            output_shape=mdp.info.action_space.size,\n",
    "                            n_actions=mdp.info.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "agent = DQN(mdp.info, pi, TorchApproximator,\n",
    "            approximator_params=approximator_params, batch_size=batch_size,\n",
    "            initial_replay_size=initial_replay_size,\n",
    "            max_replay_size=max_replay_size,\n",
    "            target_update_frequency=target_update_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training/ Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core(agent, mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect random transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "core.learn(n_steps=initial_replay_size, n_steps_per_fit=initial_replay_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:26:44 [INFO] ###################################################################################################\n",
      "14/01/2024 23:26:44 [INFO] Experiment Algorithm: DQN\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(DQN.__name__, results_dir=None)\n",
    "logger.strong_line()\n",
    "logger.info('Experiment Algorithm: ' + DQN.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the initial policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:26:46 [INFO] Epoch 0 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "pi.set_epsilon(epsilon_test)\n",
    "dataset = core.evaluate(n_steps=n_steps_test, render=False)\n",
    "J = compute_J(dataset, gamma_eval)\n",
    "logger.epoch_info(0, J=np.mean(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the initial agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"agents/acrobot_agent_initial.msh\", full_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:07<02:18,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:17:52 [INFO] Epoch 1 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:13<01:59,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:17:58 [INFO] Epoch 2 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:19<01:49,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:04 [INFO] Epoch 3 | J: -199.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:25<01:41,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:11 [INFO] Epoch 4 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:32<01:34,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:17 [INFO] Epoch 5 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:38<01:28,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:23 [INFO] Epoch 6 | J: -666.3333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:45<01:23,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:30 [INFO] Epoch 7 | J: -104.3157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:51<01:17,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:36 [INFO] Epoch 8 | J: -499.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:58<01:11,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:43 [INFO] Epoch 9 | J: -132.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:04<01:04,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:49 [INFO] Epoch 10 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:11<00:58,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:18:56 [INFO] Epoch 11 | J: -132.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:17<00:52,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:03 [INFO] Epoch 12 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:24<00:45,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:09 [INFO] Epoch 13 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:30<00:38,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:15 [INFO] Epoch 14 | J: -666.3333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [01:36<00:32,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:22 [INFO] Epoch 15 | J: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [01:43<00:25,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:28 [INFO] Epoch 16 | J: -332.6666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [01:49<00:19,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:34 [INFO] Epoch 17 | J: -499.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [01:55<00:12,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:40 [INFO] Epoch 18 | J: -94.23809523809524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [02:02<00:06,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:47 [INFO] Epoch 19 | J: -499.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:08<00:00,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/01/2024 23:19:53 [INFO] Epoch 20 | J: -82.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n in trange(n_epochs):\n",
    "    pi.set_epsilon(epsilon)\n",
    "    core.learn(n_steps=n_steps, n_steps_per_fit=train_frequency)\n",
    "    pi.set_epsilon(epsilon_test)\n",
    "    dataset = core.evaluate(n_steps=n_steps_test, render=False)\n",
    "    J = compute_J(dataset, gamma_eval)\n",
    "    logger.epoch_info(n+1, J=np.mean(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"agents/acrobot_agent_final.msh\", full_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mushroom_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
